import cv2
import torch
import numpy as np
from ultralytics import YOLO
import os
import json
import argparse

# ========== åˆå§‹åŒ–æ¨¡å‹ ==========
CALIBRATION_VERSION = 3  # bump when depth processing changes
# Detection thresholds
CONF_THRESH = 0.25
IOU_THRESH = 0.5

def initialize_models():
    print("=" * 60)
    print("ğŸš¦ éšœç¤™ç‰©åµæ¸¬ç³»çµ± - è·é›¢æ ¡æ­£ç‰ˆ")
    print("=" * 60)
    print("\nâ³ æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
    
    yolo_model = YOLO('yolov8n.pt')
    midas = torch.hub.load("intel-isl/MiDaS", "MiDaS_small")
    midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
    transform = midas_transforms.small_transform
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    midas.to(device)
    midas.eval()
    
    print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è£ç½®: {'GPU' if torch.cuda.is_available() else 'CPU'}\n")
    
    return yolo_model, midas, transform, device

# ========== Robust ROI depth helper ==========
def robust_roi_depth(depth_map, x1, y1, x2, y2, inner_ratio=0.8):
    """Compute a robust depth statistic inside a shrunken ROI.

    - Shrinks the bbox by inner_ratio to reduce edge/background contamination.
    - Filters non-finite and non-positive values.
    - Uses 20-80 percentile trimmed median for robustness.
    Returns float depth or None if insufficient data.
    """
    h, w = depth_map.shape[:2]
    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
    x1 = max(0, min(w - 1, x1))
    x2 = max(0, min(w - 1, x2))
    y1 = max(0, min(h - 1, y1))
    y2 = max(0, min(h - 1, y2))
    if x2 <= x1 or y2 <= y1:
        return None

    # shrink box
    bw = x2 - x1
    bh = y2 - y1
    pad_x = int((1.0 - inner_ratio) * 0.5 * bw)
    pad_y = int((1.0 - inner_ratio) * 0.5 * bh)
    sx1 = max(0, x1 + pad_x)
    sy1 = max(0, y1 + pad_y)
    sx2 = min(w - 1, x2 - pad_x)
    sy2 = min(h - 1, y2 - pad_y)
    if sx2 <= sx1 or sy2 <= sy1:
        return None

    region = depth_map[sy1:sy2, sx1:sx2].astype(np.float32)
    vals = region[np.isfinite(region) & (region > 0)]
    if vals.size == 0:
        return None
    q1, q3 = np.percentile(vals, [20, 80])
    trimmed = vals[(vals >= q1) & (vals <= q3)]
    if trimmed.size == 0:
        trimmed = vals
    return float(np.median(trimmed))

# ========== è¨ˆç®—æ·±åº¦å€¼ ==========
def calculate_depth(image_path, midas, transform, device, yolo_model):
    """è¨ˆç®—ç…§ç‰‡ä¸­æ‰€æœ‰ç‰©é«”çš„å¹³å‡æ·±åº¦å€¼"""
    frame = cv2.imread(image_path)
    if frame is None:
        return None, None, None
    
    # èª¿æ•´å¤§å°
    frame = cv2.resize(frame, (640, 480))
    
    # YOLOv8 åµæ¸¬
    # Standardized inference frame
    proc_w, proc_h = 640, 480
    proc_frame = cv2.resize(frame, (proc_w, proc_h))
    results = yolo_model(proc_frame, verbose=False, conf=CONF_THRESH, iou=IOU_THRESH)
    detections = results[0].boxes
    
    # MiDaS æ·±åº¦ä¼°è¨ˆ
    img_rgb = cv2.cvtColor(proc_frame, cv2.COLOR_BGR2RGB)
    input_batch = transform(img_rgb).to(device)
    
    with torch.no_grad():
        prediction = midas(input_batch)
        prediction = torch.nn.functional.interpolate(
            prediction.unsqueeze(1),
            size=img_rgb.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze()
    
    depth_map = prediction.cpu().numpy().astype(np.float32)
    
    # æ‰¾åˆ°æœ€å¤§çš„ç‰©é«”
    max_area = 0
    target_depth = None
    selected_center_x = None
    
    for box in detections:
        x1, y1, x2, y2 = map(int, box.xyxy[0])
        confidence = float(box.conf[0])
        
        if confidence < CONF_THRESH:
            continue
        
        area = (x2 - x1) * (y2 - y1)
        if area > max_area:
            max_area = area
            robust_depth = robust_roi_depth(depth_map, x1, y1, x2, y2)
            if robust_depth is not None:
                target_depth = robust_depth
                selected_center_x = (x1 + x2) / 2.0
    
    xn = None
    if selected_center_x is not None:
        frame_center_x = frame.shape[1] / 2.0
        if frame_center_x > 0:
            xn = (selected_center_x - frame_center_x) / frame_center_x
        else:
            xn = 0.0
    
    return target_depth, frame, xn

# ========== æ ¡æ­£æ¨¡å¼ ==========
def calibration_mode(yolo_model, midas, transform, device):
    """åŸ·è¡Œè·é›¢æ ¡æ­£"""
    print("=" * 60)
    print("ğŸ“ æ ¡æ­£æ¨¡å¼")
    print("=" * 60)
    print("\nèªªæ˜ï¼š")
    print("  1. è«‹æº–å‚™ä¸€å€‹ç‰©é«”ï¼ˆä¾‹å¦‚ï¼šæ¤…å­ã€ç®±å­ï¼‰")
    print("  2. å°‡ç‰©é«”æ”¾åœ¨ 3 å€‹ä¸åŒçš„å·²çŸ¥è·é›¢")
    print("  3. æ¯å€‹è·é›¢æ‹ä¸€å¼µç…§ç‰‡")
    print("  4. ç³»çµ±æœƒè‡ªå‹•è¨ˆç®—æ ¡æ­£å…¬å¼\n")
    print("å»ºè­°è·é›¢ï¼š1å…¬å°ºã€2å…¬å°ºã€3å…¬å°º")
    print("-" * 60)
    
    input("\næŒ‰ Enter é–‹å§‹æ ¡æ­£...")
    
    calibration_data = []
    existing_config = load_calibration()
    if existing_config and existing_config.get('calibration_data'):
        existing_entries = existing_config.get('calibration_data', [])
        existing_count = len(existing_entries)
        while True:
            choice = input(f"\nåµæ¸¬åˆ°æ—¢æœ‰æ ¡æ­£è³‡æ–™ï¼Œå…± {existing_count} ç­†ï¼Œæ˜¯å¦è¦åœ¨æ­¤åŸºç¤ä¸Šæ–°å¢ï¼Ÿ(Y/n): ").strip().lower()
            if choice in ("", "y", "yes"):
                calibration_data = [dict(entry) for entry in existing_entries]
                print(f"â¡ï¸  å°‡æ²¿ç”¨æ—¢æœ‰è³‡æ–™ {existing_count} ç­†ï¼Œä¸¦å†åŠ å…¥ 15 ç­†æ–°çš„æ ¡æ­£æ¨£æœ¬ã€‚")
                break
            elif choice in ("n", "no"):
                calibration_data = []
                print("â†©ï¸  å°‡æ¸…é™¤æ—¢æœ‰è³‡æ–™ä¸¦é‡æ–°é–‹å§‹æ ¡æ­£ã€‚")
                break
            else:
                print("è«‹è¼¸å…¥ Y æˆ– Nã€‚")
    else:
        print("\nç›®å‰æ²’æœ‰å·²å„²å­˜çš„æ ¡æ­£è³‡æ–™ï¼Œå°‡é‡æ–°é–‹å§‹ã€‚")
    
    for i in range(15):
        print(f"\n{'=' * 60}")
        print(f"ç¬¬ {i+1}/15 å¼µæ ¡æ­£ç…§ç‰‡")
        print("=" * 60)
        
        # è¼¸å…¥å¯¦éš›è·é›¢
        while True:
            try:
                actual_distance = float(input(f"è«‹è¼¸å…¥ç‰©é«”çš„å¯¦éš›è·é›¢ï¼ˆå…¬å°ºï¼‰: "))
                if actual_distance > 0:
                    break
                else:
                    print("âŒ è·é›¢å¿…é ˆå¤§æ–¼ 0")
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
        
        # è¼¸å…¥ç…§ç‰‡è·¯å¾‘
        while True:
            image_path = input("è«‹è¼¸å…¥ç…§ç‰‡è·¯å¾‘ï¼ˆæˆ–æ‹–æ›³ç…§ç‰‡åˆ°æ­¤ï¼‰: ").strip().strip('"').strip("'")
            
            if os.path.exists(image_path):
                depth_value, frame, xn = calculate_depth(image_path, midas, transform, device, yolo_model)
                
                if depth_value is not None:
                    calibration_data.append({
                        'distance': float(actual_distance),
                        'depth': float(depth_value),
                        'xn': float(xn) if xn is not None else 0.0
                    })
                    print(f"âœ… æˆåŠŸï¼æ·±åº¦å€¼: {depth_value:.4f}")
                    
                    # é¡¯ç¤ºç…§ç‰‡é è¦½
                    cv2.imshow(f'Calibration {i+1}', frame)
                    cv2.waitKey(1000)
                    cv2.destroyAllWindows()
                    break
                else:
                    print("âŒ ç„¡æ³•åµæ¸¬åˆ°ç‰©é«”ï¼Œè«‹é‡æ–°æ‹æ”")
            else:
                print("âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ")
    
    # ========== è¨ˆç®—æ ¡æ­£å…¬å¼ ==========
    print("\n" + "=" * 60)
    print("ğŸ”§ æ­£åœ¨è¨ˆç®—æ ¡æ­£å…¬å¼...")
    print("=" * 60)
    
    depths = np.array([d['depth'] for d in calibration_data])
    distances = np.array([d['distance'] for d in calibration_data])
    
    inv_depths = 1.0 / depths
    A = np.vstack([inv_depths, np.ones(len(inv_depths))]).T
    params, residuals, rank, s = np.linalg.lstsq(A, distances, rcond=None)

    # One-pass robust refit based on residuals (trim outliers)
    pred = params[0] * inv_depths + params[1]
    err = np.abs(pred - distances)
    med = np.median(err) if err.size else 0.0
    thresh = 2.5 * max(med, 1e-6)
    keep = err <= thresh
    if keep.sum() >= 3 and keep.sum() < len(inv_depths):
        A2 = A[keep]
        d2 = distances[keep]
        params, _, _, _ = np.linalg.lstsq(A2, d2, rcond=None)

    a, b = float(params[0]), float(params[1])

    # Fallback: if intercept leads to non-physical predictions on calibration data,
    # refit without intercept (b = 0) to enforce monotonic relation.
    pred_check = a * inv_depths + b
    if (pred_check <= 0).any() or a <= 0:
        a_numer = float(np.sum(distances * inv_depths))
        a_denom = float(np.sum(inv_depths ** 2) + 1e-9)
        a = a_numer / a_denom
        b = 0.0

    # Edge bias correction using lateral position (horizontal only, allow asymmetry)
    xns = np.array([float(d.get('xn', 0.0)) for d in calibration_data])
    d_pred = a / depths + b
    X = d_pred * xns
    Y = d_pred * (xns ** 2)
    valid = np.abs(d_pred) > 1e-6
    if np.sum(valid) >= 2 and (np.abs(X[valid]).sum() > 1e-6 or np.abs(Y[valid]).sum() > 1e-6):
        M = np.vstack([X[valid], Y[valid]]).T
        target = distances[valid] - d_pred[valid]
        corr_params, _, _, _ = np.linalg.lstsq(M, target, rcond=None)
        k1 = float(corr_params[0]) if len(corr_params) > 0 else 0.0
        k2 = float(corr_params[1]) if len(corr_params) > 1 else 0.0
    else:
        k1 = 0.0
        k2 = 0.0
    k1 = float(np.clip(k1, -0.5, 0.5))
    k2 = float(np.clip(k2, -0.5, 0.5))
    
    print("\næ ¡æ­£çµæœï¼š")
    print("-" * 60)
    for i, data in enumerate(calibration_data):
        predicted = a / data['depth'] + b
        xn_i = float(data.get('xn', 0.0))
        predicted_corr = predicted * (1.0 + k1 * xn_i + k2 * (xn_i ** 2))
        error = abs(predicted_corr - data['distance'])
        print(f"  {i+1}. å¯¦: {data['distance']:.2f}m | æ·±åº¦: {data['depth']:.4f} | é æ¸¬(æ°´å¹³æ ¡æ­£): {predicted_corr:.2f}m | èª¤å·®: {error:.2f}m")

    avg_error = np.mean([
        abs(((a / d['depth'] + b) * (1.0 + k1 * float(d.get('xn', 0.0)) + k2 * (float(d.get('xn', 0.0)) ** 2))) - d['distance'])
        for d in calibration_data
    ])
    print(f"\nå¹³å‡èª¤å·® (æ°´å¹³æ ¡æ­£å¾Œ): {avg_error:.2f}m")

    
    # å„²å­˜æ ¡æ­£è³‡æ–™
    calibration_config = {
        'version': CALIBRATION_VERSION,
        'a': a,
        'b': b,
        'k1': k1,
        'k2': k2,
        'calibration_data': calibration_data
    }
    
    try:
        with open('calibration.json', 'w', encoding='utf-8') as f:
            json.dump(calibration_config, f, indent=2, ensure_ascii=False)
        print(f"\nâœ… æ ¡æ­£å®Œæˆï¼å…¬å¼å·²å„²å­˜è‡³ calibration.json")
        print(f"   å…¬å¼: distance = {a:.4f} / depth + ({b:.4f})")
    except Exception as e:
        print(f"\nâš ï¸  å„²å­˜å¤±æ•—: {e}")
        print("ä½†å¯ä»¥ç¹¼çºŒä½¿ç”¨ï¼ˆåªæ˜¯ä¸‹æ¬¡éœ€è¦é‡æ–°æ ¡æ­£ï¼‰")
    
    return calibration_config

# ========== è¼‰å…¥æ ¡æ­£è³‡æ–™ ==========
def load_calibration():
    """è¼‰å…¥æ ¡æ­£è¨­å®š"""
    if os.path.exists('calibration.json'):
        try:
            with open('calibration.json', 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if not content:
                    print("âš ï¸  æ ¡æ­£æª”æ¡ˆæ˜¯ç©ºçš„")
                    os.remove('calibration.json')
                    return None
                data = json.loads(content)
                # Invalidate old calibration if version mismatch
                if not isinstance(data, dict) or data.get('version') != CALIBRATION_VERSION:
                    print("ï¿½`ï¿½ï¿½  ï¿½Õ¥ï¿½ï¿½ï¿½ï¿½ï¿½æª”ï¿½æ¦¡/ï¿½ï¿½ï¿½Ü¦Xï¿½Pï¿½É®×¡Aï¿½İ¨ï¿½^ï¿½sï¿½Õ¥ï¿½.")
                    try:
                        os.remove('calibration.json')
                    except:
                        pass
                    return None
                return data
        except (json.JSONDecodeError, ValueError) as e:
            print(f"âš ï¸  æ ¡æ­£æª”æ¡ˆæå£: {e}")
            print("å°‡åˆªé™¤èˆŠæª”æ¡ˆä¸¦é‡æ–°æ ¡æ­£...")
            try:
                os.remove('calibration.json')
            except:
                pass
            return None
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥å¤±æ•—: {e}")
            return None
    return None

# ========== åµæ¸¬æ¨¡å¼ ==========
def detection_mode(yolo_model, midas, transform, device, calibration_config, image_path=None, show_window=True, output_dir=None):
    """ä½¿ç”¨æ ¡æ­£å¾Œçš„å…¬å¼é€²è¡Œåµæ¸¬"""
    print("\n" + "=" * 60)
    print("ğŸ¯ åµæ¸¬æ¨¡å¼")
    print("=" * 60)
    print(f"ä½¿ç”¨æ ¡æ­£å…¬å¼: distance = {calibration_config['a']:.4f} / depth + ({calibration_config['b']:.4f})\n")
    
    WARNING_DISTANCE = 1.5
    WINDOW_WIDTH = 1280
    WINDOW_HEIGHT = 720
    
    # è¼¸å…¥ç…§ç‰‡
    provided_path = bool(image_path)
    while True:
        candidate_path = image_path if image_path else input("è«‹è¼¸å…¥è¦åµæ¸¬çš„ç…§ç‰‡è·¯å¾‘: ").strip().strip('"').strip("'")
        
        if candidate_path and os.path.exists(candidate_path):
            image_path = candidate_path
            break
        else:
            if provided_path:
                print("âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ")
                return None
            print("âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ")
    
    # è®€å–ç…§ç‰‡
    frame = cv2.imread(image_path)
    if frame is None:
        print("âŒ ç„¡æ³•è®€å–ç…§ç‰‡")
        return
    
    print(f"âœ… æˆåŠŸè®€å–: {os.path.basename(image_path)}")
    
    # èª¿æ•´å¤§å°
    # Standardized inference frame (keep fixed for YOLO/MiDaS)
    proc_w, proc_h = 640, 480
    proc_frame = cv2.resize(frame, (proc_w, proc_h))
    original_height, original_width = frame.shape[:2]
    aspect_ratio = original_width / original_height
    
    if aspect_ratio > WINDOW_WIDTH / WINDOW_HEIGHT:
        new_width = WINDOW_WIDTH
        new_height = int(WINDOW_WIDTH / aspect_ratio)
    else:
        new_height = WINDOW_HEIGHT
        new_width = int(WINDOW_HEIGHT * aspect_ratio)
    
    frame = cv2.resize(frame, (new_width, new_height))
    
    # YOLOv8 åµæ¸¬
    print("â³ æ­£åœ¨åµæ¸¬éšœç¤™ç‰©...")
    results = yolo_model(proc_frame, verbose=False, conf=CONF_THRESH, iou=IOU_THRESH)
    detections = results[0].boxes
    # Fallback: if nothing detected, try lower conf
    try:
        if detections is None or len(detections) == 0:
            results = yolo_model(proc_frame, verbose=False, conf=0.10, iou=IOU_THRESH)
            detections = results[0].boxes
            if detections is not None and len(detections) > 0:
                print("? tUXv (conf=0.10)")
    except Exception:
        pass
    
    # MiDaS æ·±åº¦ä¼°è¨ˆ
    print("â³ æ­£åœ¨ä¼°ç®—æ·±åº¦...")
    img_rgb = cv2.cvtColor(proc_frame, cv2.COLOR_BGR2RGB)
    input_batch = transform(img_rgb).to(device)
    
    with torch.no_grad():
        prediction = midas(input_batch)
        prediction = torch.nn.functional.interpolate(
            prediction.unsqueeze(1),
            size=img_rgb.shape[:2],
            mode="bicubic",
            align_corners=False,
        ).squeeze()
    
    depth_map = prediction.cpu().numpy().astype(np.float32)
    
    # è™•ç†åµæ¸¬çµæœ
    print("â³ æ­£åœ¨åˆ†æ...")
    print("\n" + "=" * 60)
    print("åµæ¸¬çµæœ")
    print("=" * 60)
    
    frame_center_x = proc_frame.shape[1] / 2
    danger_objects = []
    detection_count = 0
    
    a = calibration_config['a']
    b = calibration_config['b']
    k1 = float(calibration_config.get('k1', 0.0))
    k2 = float(calibration_config.get('k2', 0.0))
    
    for box in detections:
        x1, y1, x2, y2 = map(int, box.xyxy[0])
        confidence = float(box.conf[0])
        
        if confidence < CONF_THRESH:
            continue
        
        detection_count += 1
        center_x = (x1 + x2) / 2
        
        # è¨ˆç®—æ·±åº¦
        avg_depth = robust_roi_depth(depth_map, x1, y1, x2, y2)
        if avg_depth is None or avg_depth <= 0:
            continue
        
        # ä½¿ç”¨æ ¡æ­£å…¬å¼è¨ˆç®—è·é›¢
        if avg_depth > 0:
            distance = a / avg_depth + b
            # Lateral bias correction (horizontal angle only)
            xn = (center_x - frame_center_x) / frame_center_x if frame_center_x > 0 else 0.0
            distance = distance * (1.0 + k1 * xn + k2 * (xn ** 2))
            distance = max(0.1, min(distance, 50.0))
        else:
            distance = 999
        
        # åˆ¤æ–·é¡è‰²
        if distance < WARNING_DISTANCE:
            color = (0, 0, 255)
            status = "Danger"
        elif distance < 3.0:
            color = (0, 165, 255)
            status = "Caution"
        else:
            color = (0, 255, 0)
            status = "Safe"
        
        # ç¹ªè£½æ¡†æ¡†
        cv2.rectangle(proc_frame, (x1, y1), (x2, y2), color, 4)
        
        # é¡¯ç¤ºè·é›¢
        label = f"{distance:.1f}m"
        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1.2, 3)[0]
        cv2.rectangle(proc_frame, (x1, y1 - label_size[1] - 10), 
                      (x1 + label_size[0] + 10, y1), color, -1)
        cv2.putText(proc_frame, label, (x1 + 5, y1 - 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
        
        # çµ‚ç«¯æ©Ÿè¼¸å‡º
        print(f"\n{detection_count}. [{status}]")
        print(f"   è·é›¢: {distance:.1f}m")
        print(f"   æ·±åº¦å€¼: {avg_depth:.4f}")
        
        if distance < WARNING_DISTANCE:
            danger_objects.append({'distance': distance, 'center_x': center_x})
            print(f"   âš ï¸ è­¦å‘Š: è·é›¢éè¿‘ï¼")
    
    print("\n" + "=" * 60)
    print(f"âœ… åµæ¸¬åˆ° {detection_count} å€‹éšœç¤™ç‰©")
    
    # åˆ¤æ–·èº²é¿æ–¹å‘
    # Prepare display frame from processing frame
    frame = cv2.resize(proc_frame, (new_width, new_height))
    if danger_objects:
        print(f"âš ï¸ æœ‰ {len(danger_objects)} å€‹å±éšªéšœç¤™ç‰©")
        
        left_danger = [obj for obj in danger_objects if obj['center_x'] < frame_center_x]
        right_danger = [obj for obj in danger_objects if obj['center_x'] >= frame_center_x]
        
        if len(left_danger) < len(right_danger):
            escape_direction = "LEFT"
            arrow_start = (new_width - 100, new_height // 2)
            arrow_end = (100, new_height // 2)
        elif len(right_danger) < len(left_danger):
            escape_direction = "RIGHT"
            arrow_start = (100, new_height // 2)
            arrow_end = (new_width - 100, new_height // 2)
        else:
            left_min = min([o['distance'] for o in left_danger]) if left_danger else 999
            right_min = min([o['distance'] for o in right_danger]) if right_danger else 999
            if left_min > right_min:
                escape_direction = "LEFT"
                arrow_start = (new_width - 100, new_height // 2)
                arrow_end = (100, new_height // 2)
            else:
                escape_direction = "RIGHT"
                arrow_start = (100, new_height // 2)
                arrow_end = (new_width - 100, new_height // 2)
        
        print(f"ğŸš¨ å»ºè­°: å¾€{'å·¦' if escape_direction == 'LEFT' else 'å³'}é–ƒé¿")
        
        # ç¹ªè£½è¶…å¤§ç®­é ­
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, new_height // 2 - 80), 
                      (new_width, new_height // 2 + 80), (0, 100, 200), -1)
        frame = cv2.addWeighted(overlay, 0.5, frame, 0.5, 0)
        
        cv2.arrowedLine(frame, arrow_start, arrow_end, 
                       (0, 255, 255), 25, tipLength=0.15)
        
        text = f"GO {escape_direction}"
        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 2.5, 5)[0]
        text_x = (new_width - text_size[0]) // 2
        text_y = new_height // 2 - 100
        
        cv2.putText(frame, text, (text_x + 3, text_y + 3),
                   cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 0, 0), 5)
        cv2.putText(frame, text, (text_x, text_y),
                   cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 255, 255), 5)
        
        cv2.rectangle(frame, (0, 0), (new_width, 70), (0, 0, 200), -1)
        cv2.putText(frame, "! DANGER - OBSTACLE DETECTED !", (20, 50),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
    else:
        print("âœ… å®‰å…¨")
        cv2.rectangle(frame, (0, 0), (new_width, 70), (0, 200, 0), -1)
        cv2.putText(frame, "SAFE - No Danger", (20, 50),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 3)
    
    # é¡¯ç¤ºçµæœ
    if show_window:
        cv2.namedWindow('Obstacle Detection', cv2.WINDOW_NORMAL)
        cv2.resizeWindow('Obstacle Detection', new_width, new_height)
        cv2.imshow('Obstacle Detection', frame)
    
    # å„²å­˜çµæœ
    base_name = os.path.splitext(os.path.basename(image_path))[0]
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, f"result_{base_name}_calibrated.jpg")
    else:
        output_path = f"result_{base_name}_calibrated.jpg"
    cv2.imwrite(output_path, frame)
    print(f"ğŸ’¾ å·²å„²å­˜: {output_path}")
    
    if show_window:
        print("\næŒ‰ä»»æ„éµé—œé–‰...")
        cv2.waitKey(0)
        cv2.destroyAllWindows()
    else:
        cv2.destroyAllWindows()
    return output_path

# ========== æ‰¹æ¬¡è™•ç† ==========
def batch_process_folder(input_dir, output_dir, yolo_model, midas, transform, device, calibration_config, show_window=False):
    """Process every image in input_dir and save annotated results to output_dir."""
    print("\n" + "=" * 60)
    print("ğŸ“‚ æ‰¹æ¬¡åµæ¸¬æ¨¡å¼")
    print("=" * 60)
    print(f"ä¾†æºè³‡æ–™å¤¾: {input_dir}")
    print(f"è¼¸å‡ºè³‡æ–™å¤¾: {output_dir}")
    
    if not os.path.isdir(input_dir):
        print("âŒ æ‰¾ä¸åˆ°ä¾†æºè³‡æ–™å¤¾")
        return
    
    os.makedirs(output_dir, exist_ok=True)
    supported_ext = (".jpg", ".jpeg", ".png", ".bmp")
    image_files = [
        os.path.join(input_dir, f)
        for f in os.listdir(input_dir)
        if f.lower().endswith(supported_ext)
    ]
    
    if not image_files:
        print("â„¹ï¸ è³‡æ–™å¤¾ä¸­æ²’æœ‰å½±åƒæª”")
        return
    
    image_files.sort()
    
    for idx, img_path in enumerate(image_files, start=1):
        print(f"\n[{idx}/{len(image_files)}] è™•ç† {os.path.basename(img_path)}")
        detection_mode(
            yolo_model,
            midas,
            transform,
            device,
            calibration_config,
            image_path=img_path,
            show_window=show_window,
            output_dir=output_dir,
        )

# ========== ä¸»ç¨‹å¼ ==========
def main():
    parser = argparse.ArgumentParser(description="Obstacle detection / calibration tool")
    parser.add_argument("--batch", "-b", type=str, help="æŒ‡å®šè³‡æ–™å¤¾å¾Œæ‰¹æ¬¡è™•ç†å…¶ä¸­çš„å½±åƒ")
    parser.add_argument("--output", "-o", type=str, default="result", help="è¼¸å‡ºæ¨™è¨»å½±åƒçš„è³‡æ–™å¤¾")
    parser.add_argument("--nogui", action="store_true", help="æ‰¹æ¬¡æ¨¡å¼ä¸‹ä¸é¡¯ç¤º OpenCV è¦–çª—")
    args = parser.parse_args()
    
    yolo_model, midas, transform, device = initialize_models()
    
    calibration_config = load_calibration()
    
    # æ‰¹æ¬¡æ¨¡å¼ï¼šæœ‰æŒ‡å®š batch åƒæ•¸æ™‚ç›´æ¥è™•ç†è³‡æ–™å¤¾ä¸¦çµæŸ
    if args.batch:
        if not calibration_config:
            print("âŒ æœªæ‰¾åˆ°æ ¡æ­£è³‡æ–™ï¼Œéœ€è¦å…ˆé€²è¡Œæ ¡æ­£")
            return
        batch_process_folder(
            args.batch,
            args.output,
            yolo_model,
            midas,
            transform,
            device,
            calibration_config,
            show_window=not args.nogui,
        )
        return
    
    if calibration_config:
        print("âœ… æ‰¾åˆ°æ ¡æ­£è³‡æ–™")
        print(f"   å…¬å¼: distance = {calibration_config['a']:.4f} / depth + ({calibration_config['b']:.4f})")
        print("\nè«‹é¸æ“‡æ¨¡å¼ï¼š")
        print("  1. é‡æ–°æ ¡æ­£")
        print("  2. ä½¿ç”¨ç¾æœ‰æ ¡æ­£é€²è¡Œåµæ¸¬")
        choice = input("\nè¼¸å…¥é¸é … (1/2): ").strip()
        
        if choice == "1":
            calibration_config = calibration_mode(yolo_model, midas, transform, device)
    else:
        print("âŒ æœªæ‰¾åˆ°æ ¡æ­£è³‡æ–™ï¼Œéœ€è¦å…ˆé€²è¡Œæ ¡æ­£")
        calibration_config = calibration_mode(yolo_model, midas, transform, device)
    
    # é€²å…¥åµæ¸¬æ¨¡å¼
    while True:
        detection_mode(yolo_model, midas, transform, device, calibration_config)
        
        again = input("\nè¦ç¹¼çºŒåµæ¸¬å…¶ä»–ç…§ç‰‡å—? (y/n): ").lower()
        if again != 'y':
            break
    
    print("\nâœ… ç¨‹å¼çµæŸ")

if __name__ == "__main__":
    main()










